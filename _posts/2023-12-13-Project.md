# Individual Optional Project

This document will try to explain the coding behind the provided jupyter notebook which participated in the Blue Book for Bulldozers Kaggle Project 

 {:toc}

## About the project

This Kaggle project is about to predict the sale price of a particular piece of heavy equiment at auction based on it's usage, equipment type, and configuaration.  The data is sourced from auction result postings and includes information on usage and equipment configurations.

The fictional enterprise name is ´Fast Iron´ and the data provided has been normalized and the goal of the enterprise is to create a "blue book for bull dozers," for customers to value what their heavy equipment fleet is worth at auction.

Something that is mentioned in the competition is to evaluate the RMSLE((root mean squared log error) between the actual and predicted auction prices.

[link to Kaggle](https://www.kaggle.com/c/bluebook-for-bulldozers)

## Description of the dataset

The dataset contains the following fields:

'SalesID', 'SalePrice', 'MachineID', 'ModelID', 'datasource',
 'auctioneerID', 'YearMade', 'MachineHoursCurrentMeter', 'UsageBand',
 'saledate', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc',
 'fiModelSeries', 'fiModelDescriptor', 'ProductSize',
 'fiProductClassDesc', 'state', 'ProductGroup', 'ProductGroupDesc',
 'Drive_System', 'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control',
 'Stick', 'Transmission', 'Turbocharged', 'Blade_Extension',
 'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics',
 'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control', 'Tire_Size',
 'Coupler', 'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow',
 'Track_Type', 'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb',
 'Pattern_Changer', 'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type',
 'Travel_Controls', 'Differential_Type', 'Steering_Controls'

 From which some important variables are:
 
 **| Field | Description |**
 | SalePrice | Contains the values of the price in the auction of the tractor |
 | SalesID | unique identifier of the sale |
 | MachineID |unique identifier of a machine |
 | saledate | date of the sale |
 | Yearmade | Year when the machine was created |
 |UsageBand|Classification of the usage of the machine (Low,medium, high)|

 Also, some additional detail to put is the author of the coding create some functions to create some data arrangement on the dataset. 
 Specially on one column that have some timestamp value, and the coder decided to devide this column in various columns.
 
 In here it will be detailed some of the functions that are going to be used 

**add_datepart**

   add_datepart converts a column of df from a datetime64 to many columns containing
    the information from the date.
    
 ```python
# Prints 
def add_datepart(df, fldnames, drop=True, time=False, errors="raise"):
if isinstance(fldnames,str):
        fldnames = [fldnames]
    for fldname in fldnames:
        fld = df[fldname] # fld = df['saledate']
        fld_dtype = fld.dtype
        if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):
            fld_dtype = np.datetime64

        if not np.issubdtype(fld_dtype, np.datetime64):
            df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True, errors=errors)
        targ_pre = re.sub('[Dd]ate$', '', fldname)
        attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',
                'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']
        if time: attr = attr + ['Hour', 'Minute', 'Second']
        for n in attr: 
            if n == 'Week':
                df[targ_pre + n] = getattr(fld.dt.isocalendar(),'week')
            else:
                df[targ_pre + n] = getattr(fld.dt, n.lower())
        df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9
        if drop: df.drop(fldname, axis=1, inplace=True)

```

**proc_df**

proc_df takes a data frame df and splits off the response variable, and
    changes the df into an entirely numeric dataframe. For each column of df
    which is not in skip_flds nor in ignore_flds, na values are replaced by the
    median value of the column. 
 This function is goinf to be used to do some data cleaning

 {% include info.html text="This function is goinf to be used to do some data cleaning. The criteria used to replace the missing values is with the
 median. However it can be applied other techniques as average or SMOTEK" %}

   ```python
    
    def proc_df(df, y_fld=None, skip_flds=None, ignore_flds=None, do_scale=False, na_dict=None,
            preproc_fn=None, max_n_cat=None, subset=None, mapper=None):
    if not ignore_flds: ignore_flds=[]
    if not skip_flds: skip_flds=[]
    if subset: df = get_sample(df, subset)
    else: df = df.copy()
    ignored_flds = df.loc[:, ignore_flds]
    df = df.drop(ignore_flds, axis = 1, inplace = False)
    if preproc_fn: preproc_fn(df)
    if y_fld is None: y = None
    else:
        if not is_numeric_dtype(df[y_fld]): df[y_fld] = pd.Categorical(df[y_fld]).codes
        y = df[y_fld].values
        skip_flds += [y_fld]
    df.drop(skip_flds, axis=1, inplace=True)

    if na_dict is None: na_dict = {}
    else: na_dict = na_dict.copy()
    na_dict_initial = na_dict.copy()
    for n,c in df.items(): na_dict = fix_missing(df, c, n, na_dict)
    if len(na_dict_initial.keys()) > 0:
        df.drop([a + '_na' for a in list(set(na_dict.keys()) - set(na_dict_initial.keys()))], axis=1, inplace=True)
    if do_scale: mapper = scale_vars(df, mapper)
    for n,c in df.items(): numericalize(df, c, n, max_n_cat)
    df = pd.get_dummies(df, dummy_na=True)
    df = pd.concat([ignored_flds, df], axis=1)
    res = [df, y, na_dict]
    if do_scale: res = res + [mapper]
    return res

   ```

## Exploring the data 

 One thing to point out on the document is that the author of the coding don't have a section to provide of visual 
 explorations and some statistical exploration on the data. 
 In here it is going to be created an univariate and bivariate analysis to check on the data 
 
 **Missing values**

 In the next piece of code is being calculated the ratio of missing values that the dataset have:
 
 ```python
    missing_percentage = (df_raw.isna().sum() / len(df_raw)) * 100
    missing_percentage_sorted = missing_percentage.sort_values(ascending=False)
    print(missing_percentage_sorted)
 ```

In the image it can be visualize that there are several columns that have a high ratio of blank values. For example: Engine_Horsepower with almost no data in the field (93% blank)
 
[![/images/image_columns_ratio_blanks.png]]



## Data Cleaning
## Featuring Engineering
## Modeling 
## Regression Model
## Decision Tree Model
## Ensambling Methods
## Aditional aspects: about classification modeling




