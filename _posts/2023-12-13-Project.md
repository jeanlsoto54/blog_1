# Individual Optional Project

This document will try to explain the coding behind the provided jupyter notebook which participated in the Blue Book for Bulldozers Kaggle Project 

1. TOC
{:toc}

## About the project

This Kaggle project is about to predict the sale price of a particular piece of heavy equiment at auction based on it's usage, equipment type, and configuaration.  The data is sourced from auction result postings and includes information on usage and equipment configurations.

The fictional enterprise name is ´Fast Iron´ and the data provided has been normalized and the goal of the enterprise is to create a "blue book for bull dozers," for customers to value what their heavy equipment fleet is worth at auction.

Something that is mentioned in the competition is to evaluate the RMSLE(root mean squared log error) between the actual and predicted auction prices.

[link to Kaggle](https://www.kaggle.com/c/bluebook-for-bulldozers)

## Description of the dataset

The dataset contains the following fields:

'SalesID', 'SalePrice', 'MachineID', 'ModelID', 'datasource',
 'auctioneerID', 'YearMade', 'MachineHoursCurrentMeter', 'UsageBand',
 'saledate', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc',
 'fiModelSeries', 'fiModelDescriptor', 'ProductSize',
 'fiProductClassDesc', 'state', 'ProductGroup', 'ProductGroupDesc',
 'Drive_System', 'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control',
 'Stick', 'Transmission', 'Turbocharged', 'Blade_Extension',
 'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics',
 'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control', 'Tire_Size',
 'Coupler', 'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow',
 'Track_Type', 'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb',
 'Pattern_Changer', 'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type',
 'Travel_Controls', 'Differential_Type', 'Steering_Controls'

 From which some important variables are:
 
 **| Field | Description |**
 | SalePrice | Contains the values of the price in the auction of the tractor |
 | SalesID | unique identifier of the sale |
 | MachineID |unique identifier of a machine |
 | saledate | date of the sale |
 | Yearmade | Year when the machine was created |
 |UsageBand|Classification of the usage of the machine (Low,medium, high)|

 Also, some additional detail to put is the author of the coding create some functions to create some data arrangement on the dataset. 
 Specially on one column that have some timestamp value, and the coder decided to devide this column in various columns.
 
 In here it will be detailed some of the functions that are going to be used 

**add_datepart**

   add_datepart converts a column of df from a datetime64 to many columns containing
    the information from the date.
    
 ```python
# Prints 
def add_datepart(df, fldnames, drop=True, time=False, errors="raise"):
if isinstance(fldnames,str):
        fldnames = [fldnames]
    for fldname in fldnames:
        fld = df[fldname] # fld = df['saledate']
        fld_dtype = fld.dtype
        if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):
            fld_dtype = np.datetime64

        if not np.issubdtype(fld_dtype, np.datetime64):
            df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True, errors=errors)
        targ_pre = re.sub('[Dd]ate$', '', fldname)
        attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',
                'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']
        if time: attr = attr + ['Hour', 'Minute', 'Second']
        for n in attr: 
            if n == 'Week':
                df[targ_pre + n] = getattr(fld.dt.isocalendar(),'week')
            else:
                df[targ_pre + n] = getattr(fld.dt, n.lower())
        df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9
        if drop: df.drop(fldname, axis=1, inplace=True)

```

**proc_df**

proc_df takes a data frame df and splits off the response variable, and
    changes the df into an entirely numeric dataframe. For each column of df
    which is not in skip_flds nor in ignore_flds, na values are replaced by the
    median value of the column. 
 This function is goinf to be used to do some data cleaning

 {% include info.html text="This function is goinf to be used to do some data cleaning. The criteria used to replace the missing values is with the
 median. However it can be applied other techniques as average or SMOTEK" %}

   ```python
    
    def proc_df(df, y_fld=None, skip_flds=None, ignore_flds=None, do_scale=False, na_dict=None,
            preproc_fn=None, max_n_cat=None, subset=None, mapper=None):
    if not ignore_flds: ignore_flds=[]
    if not skip_flds: skip_flds=[]
    if subset: df = get_sample(df, subset)
    else: df = df.copy()
    ignored_flds = df.loc[:, ignore_flds]
    df = df.drop(ignore_flds, axis = 1, inplace = False)
    if preproc_fn: preproc_fn(df)
    if y_fld is None: y = None
    else:
        if not is_numeric_dtype(df[y_fld]): df[y_fld] = pd.Categorical(df[y_fld]).codes
        y = df[y_fld].values
        skip_flds += [y_fld]
    df.drop(skip_flds, axis=1, inplace=True)

    if na_dict is None: na_dict = {}
    else: na_dict = na_dict.copy()
    na_dict_initial = na_dict.copy()
    for n,c in df.items(): na_dict = fix_missing(df, c, n, na_dict)
    if len(na_dict_initial.keys()) > 0:
        df.drop([a + '_na' for a in list(set(na_dict.keys()) - set(na_dict_initial.keys()))], axis=1, inplace=True)
    if do_scale: mapper = scale_vars(df, mapper)
    for n,c in df.items(): numericalize(df, c, n, max_n_cat)
    df = pd.get_dummies(df, dummy_na=True)
    df = pd.concat([ignored_flds, df], axis=1)
    res = [df, y, na_dict]
    if do_scale: res = res + [mapper]
    return res

   ```
**train_cats**

Change any columns of strings in a panda's dataframe to a column of
    categorical values. This applies the changes inplace

```python
    def train_cats(df):
         for n, c in df.items():
        if is_string_dtype(c): df[n] = c.astype('category').cat.as_ordered()
```
## Featuring Engineering

 One thing to point out on the document is that the author of the coding don't have a section to provide of visual 
 explorations and some statistical exploration on the data. 
 In here it is going to be created an univariate and bivariate analysis to check on the data 
 
 **Missing values**

 In the next piece of code is being calculated the ratio of missing values that the dataset have:
 
 ```python
    missing_percentage = (df_raw.isna().sum() / len(df_raw)) * 100
    missing_percentage_sorted = missing_percentage.sort_values(ascending=False)
    print(missing_percentage_sorted)
 ```

In the image it can be visualize that there are several columns that have a high ratio of blank values. For example: Engine_Horsepower with almost no data in the field (93% blank)
 
![prueba!](/images/IA_im1.PNG "blank columns")

As can be seen in the table the columns have no values
![](/images/IA_im2.PNG )

From all the columns (52) that the dataset presents . Only 15 features provide almost the whole data for each case (blanks <5%). The remaining columns have serious issue with no data (20% < blank < 93%)

{% include info.html text="For what is being discovered in this step, something that is recommended to consider in the problem is to only keep the features that provide complete data for the modeling. Only keep with the 15 columns" %}

**Data Cleaning**

The mentioned features to maintain for the modeling are the next ones. This is a recomendation
to improve the dataset.

 ```python
df_raw[['Hydraulics','auctioneerID','Enclosure','ProductGroupDesc','ProductGroup','state','fiProductClassDesc','SalePrice','fiBaseModel','fiModelDesc','saledate'
        ,'YearMade','datasource','ModelID','MachineID','SalesID']]
```

![](/images/IA_im3.PNG "value of the first row of the keept features")

However, the creator of the jupyter notebook decided to not apply some cleaning in the data; as outliers
or any wrong features or reduce the dimentions on the dataset, and applied directly a change of the categorical values in numerical features so the model can train with it.

Firstly, apply the created function **train_cats** to convert any string feature in a categorical features

```python
    train_cats(df_raw)
```

After that applies the **proc_df** command to convert all the variables in numerical, separate the Y value to be predicted, finally complete the blank values with the median value of each feature

{% include alert.html text="It is wrong assumption to apply the median value for the blanks in the dataset since there are various features that hold almost no data." %}


```python
    df, y, nas = proc_df(df_raw, 'SalePrice')
 ```


**Univariate statistics**

Now we are going to separate the data between the quantitative and cualitative data and check how these features are projected. For this we separate in the class objects to were this features belong:

```python
    df_cat = proposed_df_raw.select_dtypes(include=['object'])
    df_num = proposed_df_raw.select_dtypes(include=['int64','float64'])
 ```
For the cuantitative variables the next piece code is run so it can be displayed the distribution of them,

```python
#for cuantitative features 
for col in df_num.columns:
    fig, ax = plt.subplots(1, 2, figsize=(10, 4))

    # Histogram
    sns.histplot(df_num[col], kde=True, bins=20, ax=ax[0])
    ax[0].set_title(f'Histogram for {col}')
    ax[0].set_xlabel(col)
    ax[0].set_ylabel('Frequency')

    # Box plot
    sns.boxplot(x=df_num[col], ax=ax[1])
    ax[1].set_title(f'Boxplot for {col}')
    ax[1].set_xlabel(col)

    plt.tight_layout()
    plt.show()
```

From which we can state the next bullets:
  + Some of these cuantitative features seems to be not relevant to the study as  SalesID,which are key identifiers that are going to be different for every row.
  + There are some wrong values in the the data of Year. There are some values to be reported to be around year 1000, which we will recommend to take out of the data.
  + MachineID maybe good to be classified as a categorical feature, and later apply some OneHotEncoding with this field.

![](/images/IA_im5.png "graph were is showed the distribution of Year")


**Bivariate statistics**

Now we are going to check which are the correlations that each of the quantitative variables have 
between each other:

 

```python
    import seaborn as sns
    sns.pairplot(df_num)
 ```

![](/images/IA_im4.png "Correlation of the following numerical features")

**Fetching new features**

Something that is proposed by the author of the jupyter notebook is to divide the column 'SaleDate'
in several date values as Month, day, quarter and so on.
This is achieved with the call of the function add_datepart, previously depicted. 

```python
add_datepart(df_raw, 'saledate')
 ```

 The result is that 13 new features are created in the dataset

![](/images/IA_im6.PNG "New columns painted in yellow")


## Modeling 

**Preparing the dataset**

The variable that is needed to predict in the model is Sale Price. which is indicating
how much was money that was payed for the machine in the historical auctions.

The Kaggle competence ask that the results should be placed in a logaritmic dimention.
That is why the variable Y is converted to this form with this code.

```python
df_raw.SalePrice = np.log(df_raw.SalePrice)
```

Also, the dataset needs to be split in order to apply the proper workflow of learning and testing
with different dataserts. The author do it in the next way

```python
n_total = len(df)
n_valid = 12000  # same as Kaggle's test set size
n_train = n_total - n_valid
n_small = 20000

print('full number of data points : {}'.format(n_total))
print('number of validation data points : {}'.format(n_valid))
print('number of training data points : {}'.format(n_train))
print('number of subsampled training points : {}'.format(n_small))

X_train, X_valid = split_vals(df, n_train)
y_train, y_valid = split_vals(y, n_train)

X_small, _ = split_vals(df, n_small)
y_small, _ = split_vals(y, n_small)

print('Number of small training data points: X = {}, y = {}'.format(X_small.shape, y_small.shape))
print('Number of full training data points: X = {}, y = {}'.format(X_train.shape, y_train.shape))
print('Number of validation data points: X = {}, y = {}'.format(X_valid.shape, y_valid.shape))
```
![](/images/IA_im7.PNG )

Finally to measure the results in the next sections the author is creating some functions to it. This codes run the model inside and provide with the statistic results: RMSE, R square, Out of the Bag score (related to the decision tree)

```python
def rmse(y_gold, y_pred):
    return math.sqrt(((y_gold - y_pred)**2).mean())

def print_score(m, X_train, y_train, X_valid, y_valid):
    print('RMSE on train set: {:.4f}'.format(rmse(m.predict(X_train), y_train)))
    print('RMSE on valid set: {:.4f}'.format(rmse(m.predict(X_valid), y_valid)))
    print('R^2 on train set: {:.4f}'.format(m.score(X_train, y_train)))
    print('R^2 on valid set: {:.4f}'.format(m.score(X_valid, y_valid)))
    if hasattr(m, 'oob_score_'): print('R^2 on oob set: {:.4f}'.format(m.oob_score_))
    return
```
**Decision Tree Model**

The author decides that the main model to try in this exercise is Decision Tree. First start with a simple Decision Tree

```python
# model on small training data
base_model = RandomForestRegressor(n_estimators = 10, n_jobs = -1, random_state = 42, oob_score=True)

%time base_model.fit(X_small, y_small)
print_score(base_model, X_small, y_small, X_valid, y_valid)
```
![](/images/IA_im8.PNG )


Then starts to propose several decision trees, adjusting the parameters

In the case 1 the next parameter were set:
 + splitter: 
 + criterion: the square error is used to define when to split a node
 + max_depth: maximum deph of a tree should be 3 levels
 + min_samples_split: it should be required a minimun of 2 samples in order to split
 

```python
#CASE 1
model_dt = DecisionTreeRegressor(
    criterion = 'squared_error', # 'mse',
    splitter = 'best',
    max_depth = 3, # None
    min_samples_split = 2,
    min_samples_leaf = 1,
    min_weight_fraction_leaf = 0.0,
    max_features = None,#'auto',
    max_leaf_nodes = None,
    min_impurity_decrease = 0.0,
    ccp_alpha = 0.0,
    random_state = 42, # None,
)
```
![](/images/IA_im9.PNG )

From the result:
 + The model is not predicting well, with only a 50% of chance to predict the price of the auction  

Tree created: 

![](/images/IA_im10.png )



In the case 2 the next parameter were set:
 + n_estimators: 1 tree to create
 + criterion: the square error is used to define when to split a node
 + max_depth: maximum deph of a tree should be 3 levels
 + min_samples_split: it should be required a minimun of 2 samples in order to split
 + min_impurity_decrease: the gini impurity should reduce in 0, (not good)

```python
#CASE 2
model_rf = RandomForestRegressor(
    # parameters shared with DecisionTreeRegressor
    n_estimators = 1, # 100 # 1 = the model is a single tree
    criterion = 'squared_error',
    max_depth = 3, # None
    min_samples_split = 2,
    min_samples_leaf = 1,
    min_weight_fraction_leaf = 0.0,
    max_features = None,#'auto',
    max_leaf_nodes = None,
    min_impurity_decrease = 0.0,
    ccp_alpha = 0.0,
    random_state = 42, # None,

    # RandomForestRegressor specific hyperparameters
    bootstrap = True, # default = True
    oob_score = True,
    max_samples = None,

    # extra parameters
    warm_start = False,
    n_jobs = -1, # None
    verbose = 0,
)
```

![](/images/IA_im11.PNG )

From the result:
 + Also, in this proposal, the model is doing right in the model having similar results

```python
#CASE 3
model_dt = DecisionTreeRegressor(
    criterion = 'squared_error', # 'mse',
    splitter = 'best',
    max_depth = None,
    min_samples_split = 2,
    min_samples_leaf = 1,
    min_weight_fraction_leaf = 0.0,
    max_features = None,#'auto',
    max_leaf_nodes = None,
    min_impurity_decrease = 0.0,
    random_state = 42, # None,
    ccp_alpha = 0.0,
)
```

**Ensambling Methods**
## Aditional aspects: about classification modeling

The jupyter notebook provided has several coding related to a classification modeling that the autor wants to achieve. Since this is out of the scope of the Kaggle competition is not going to be addressed in this file.




