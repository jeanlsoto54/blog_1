# Individual Optional Project

This document will try to explain the coding behind the provided jupyter notebook which participated in the Blue Book for Bulldozers Kaggle Project 

1. TOC
{:toc}

## About the project

This Kaggle project is about to predict the sale price of a particular piece of heavy equiment at auction based on it's usage, equipment type, and configuaration.  The data is sourced from auction result postings and includes information on usage and equipment configurations.

The fictional enterprise name is ´Fast Iron´ and the data provided has been normalized and the goal of the enterprise is to create a "blue book for bull dozers," for customers to value what their heavy equipment fleet is worth at auction.

Something that is mentioned in the competition is to evaluate the RMSLE(root mean squared log error) between the actual and predicted auction prices.

[link to Kaggle](https://www.kaggle.com/c/bluebook-for-bulldozers)

## Description of the dataset

The dataset contains the following fields:

'SalesID', 'SalePrice', 'MachineID', 'ModelID', 'datasource',
 'auctioneerID', 'YearMade', 'MachineHoursCurrentMeter', 'UsageBand',
 'saledate', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc',
 'fiModelSeries', 'fiModelDescriptor', 'ProductSize',
 'fiProductClassDesc', 'state', 'ProductGroup', 'ProductGroupDesc',
 'Drive_System', 'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control',
 'Stick', 'Transmission', 'Turbocharged', 'Blade_Extension',
 'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics',
 'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control', 'Tire_Size',
 'Coupler', 'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow',
 'Track_Type', 'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb',
 'Pattern_Changer', 'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type',
 'Travel_Controls', 'Differential_Type', 'Steering_Controls'

 From which some important variables are:
 
 **| Field | Description |**
 | SalePrice | Contains the values of the price in the auction of the tractor |
 | SalesID | unique identifier of the sale |
 | MachineID |unique identifier of a machine |
 | saledate | date of the sale |
 | Yearmade | Year when the machine was created |
 |UsageBand|Classification of the usage of the machine (Low,medium, high)|

 Also, some additional detail to put is the author of the coding create some functions to create some data arrangement on the dataset. 
 Specially on one column that have some timestamp value, and the coder decided to devide this column in various columns.
 
 In here it will be detailed some of the functions that are going to be used 

**add_datepart**

   add_datepart converts a column of df from a datetime64 to many columns containing
    the information from the date.
    
 ```python
# Prints 
def add_datepart(df, fldnames, drop=True, time=False, errors="raise"):
if isinstance(fldnames,str):
        fldnames = [fldnames]
    for fldname in fldnames:
        fld = df[fldname] # fld = df['saledate']
        fld_dtype = fld.dtype
        if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):
            fld_dtype = np.datetime64

        if not np.issubdtype(fld_dtype, np.datetime64):
            df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True, errors=errors)
        targ_pre = re.sub('[Dd]ate$', '', fldname)
        attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',
                'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']
        if time: attr = attr + ['Hour', 'Minute', 'Second']
        for n in attr: 
            if n == 'Week':
                df[targ_pre + n] = getattr(fld.dt.isocalendar(),'week')
            else:
                df[targ_pre + n] = getattr(fld.dt, n.lower())
        df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9
        if drop: df.drop(fldname, axis=1, inplace=True)

```

**proc_df**

proc_df takes a data frame df and splits off the response variable, and
    changes the df into an entirely numeric dataframe. For each column of df
    which is not in skip_flds nor in ignore_flds, na values are replaced by the
    median value of the column. 
 This function is goinf to be used to do some data cleaning

 {% include info.html text="This function is goinf to be used to do some data cleaning. The criteria used to replace the missing values is with the
 median. However it can be applied other techniques as average or SMOTEK" %}

   ```python
    
    def proc_df(df, y_fld=None, skip_flds=None, ignore_flds=None, do_scale=False, na_dict=None,
            preproc_fn=None, max_n_cat=None, subset=None, mapper=None):
    if not ignore_flds: ignore_flds=[]
    if not skip_flds: skip_flds=[]
    if subset: df = get_sample(df, subset)
    else: df = df.copy()
    ignored_flds = df.loc[:, ignore_flds]
    df = df.drop(ignore_flds, axis = 1, inplace = False)
    if preproc_fn: preproc_fn(df)
    if y_fld is None: y = None
    else:
        if not is_numeric_dtype(df[y_fld]): df[y_fld] = pd.Categorical(df[y_fld]).codes
        y = df[y_fld].values
        skip_flds += [y_fld]
    df.drop(skip_flds, axis=1, inplace=True)

    if na_dict is None: na_dict = {}
    else: na_dict = na_dict.copy()
    na_dict_initial = na_dict.copy()
    for n,c in df.items(): na_dict = fix_missing(df, c, n, na_dict)
    if len(na_dict_initial.keys()) > 0:
        df.drop([a + '_na' for a in list(set(na_dict.keys()) - set(na_dict_initial.keys()))], axis=1, inplace=True)
    if do_scale: mapper = scale_vars(df, mapper)
    for n,c in df.items(): numericalize(df, c, n, max_n_cat)
    df = pd.get_dummies(df, dummy_na=True)
    df = pd.concat([ignored_flds, df], axis=1)
    res = [df, y, na_dict]
    if do_scale: res = res + [mapper]
    return res

   ```
**train_cats**

Change any columns of strings in a panda's dataframe to a column of
    categorical values. This applies the changes inplace

```python
    def train_cats(df):
         for n, c in df.items():
        if is_string_dtype(c): df[n] = c.astype('category').cat.as_ordered()
```
## Featuring Engineering

 One thing to point out on the document is that the author of the coding don't have a section to provide of visual 
 explorations and some statistical exploration on the data. 
 In here it is going to be created an univariate and bivariate analysis to check on the data 
 
 **Missing values**

 In the next piece of code is being calculated the ratio of missing values that the dataset have:
 
 ```python
    missing_percentage = (df_raw.isna().sum() / len(df_raw)) * 100
    missing_percentage_sorted = missing_percentage.sort_values(ascending=False)
    print(missing_percentage_sorted)
 ```

In the image it can be visualize that there are several columns that have a high ratio of blank values. For example: Engine_Horsepower with almost no data in the field (93% blank)
 
![prueba!](/images/IA_im1.PNG "blank columns")

As can be seen in the table the columns have no values
![](/images/IA_im2.PNG )

From all the columns (52) that the dataset presents . Only 15 features provide almost the whole data for each case (blanks <5%). The remaining columns have serious issue with no data (20% < blank < 93%)

{% include info.html text="For what is being discovered in this step, something that is recommended to consider in the problem is to only keep the features that provide complete data for the modeling. Only keep with the 15 columns" %}

**Data Cleaning**

The mentioned features to maintain for the modeling are the next ones. This is a recomendation
to improve the dataset.

 ```python
df_raw[['Hydraulics','auctioneerID','Enclosure','ProductGroupDesc','ProductGroup','state','fiProductClassDesc','SalePrice','fiBaseModel','fiModelDesc','saledate'
        ,'YearMade','datasource','ModelID','MachineID','SalesID']]
```

![](/images/IA_im3.PNG "value of the first row of the keept features")

However, the creator of the jupyter notebook decided to not apply some cleaning in the data; as outliers
or any wrong features or reduce the dimentions on the dataset, and applied directly a change of the categorical values in numerical features so the model can train with it.

Firstly, apply the created function **train_cats** to convert any string feature in a categorical features

```python
    train_cats(df_raw)
```

After that applies the **proc_df** command to convert all the variables in numerical, separate the Y value to be predicted, finally complete the blank values with the median value of each feature

{% include alert.html text="It is wrong assumption to apply the median value for the blanks in the dataset since there are various features that hold almost no data." %}


```python
    df, y, nas = proc_df(df_raw, 'SalePrice')
 ```


**Univariate statistics**

Now we are going to separate the data between the quantitative and cualitative data and check how these features are projected. For this we separate in the class objects to were this features belong:

```python
    df_cat = proposed_df_raw.select_dtypes(include=['object'])
    df_num = proposed_df_raw.select_dtypes(include=['int64','float64'])
 ```
For the cuantitative variables the next piece code is run so it can be displayed the distribution of them,

```python
#for cuantitative features 
for col in df_num.columns:
    fig, ax = plt.subplots(1, 2, figsize=(10, 4))

    # Histogram
    sns.histplot(df_num[col], kde=True, bins=20, ax=ax[0])
    ax[0].set_title(f'Histogram for {col}')
    ax[0].set_xlabel(col)
    ax[0].set_ylabel('Frequency')

    # Box plot
    sns.boxplot(x=df_num[col], ax=ax[1])
    ax[1].set_title(f'Boxplot for {col}')
    ax[1].set_xlabel(col)

    plt.tight_layout()
    plt.show()
```

From which we can state the next bullets:
  + Some of these cuantitative features seems to be not relevant to the study as  SalesID,which are key identifiers that are going to be different for every row.
  + There are some wrong values in the the data of Year. There are some values to be reported to be around year 1000, which we will recommend to take out of the data.
  + MachineID maybe good to be classified as a categorical feature, and later apply some OneHotEncoding with this field.

![](/images/IA_im5.PNG "graph were is showed the distribution of Year")


**Bivariate statistics**

Now we are going to check which are the correlations that each of the quantitative variables have 
between each other:



```python
    import seaborn as sns
    sns.pairplot(df_num)
 ```

![](/images/IA_im4.PNG "Correlation of the following numerical features")

## Modeling 
## Regression Model
## Decision Tree Model
## Ensambling Methods
## Aditional aspects: about classification modeling

The jupyter notebook provided has several coding related to a classification modeling that the autor wants to achieve. Since this is out of the scope of the Kaggle competition is not going to be addressed in this file.




